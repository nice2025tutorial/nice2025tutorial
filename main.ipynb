{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end development/deployment toolchain tutorial\n",
    "\n",
    "This tutorial provides a comprehensive walkthrough of our toolchain, designed to streamline the development and deployment of Spiking Neural Networks (SNNs) onto our (unreleased) custom event-based neuromorphic accelerator hardware. Attendees will gain hands-on experience with model configuration, training, quantization, and hardware export, enabling them to optimize neural networks for real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from extras import set_random_seed\n",
    "set_random_seed(hash(\"NICEWorkshop2025\") % (2**32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecs_train.config import load_yaml\n",
    "from ecs_train.training import initialize_experiment\n",
    "from ecs_train.visualization import network_logger, draw_perf_log\n",
    "from ecs_train.utils import export_nir\n",
    "from ecs_common.hardware_config import from_metadata\n",
    "\n",
    "from extras import check_constraints, quantize_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "With our toolchain being an end-to-end frame work for developing and deploying SNNs, an integral part of this is the training of neural networks. In this case, we use PyTorch Lightning and Norse under the hood. However, the training framework is interchangeable, as long as it supports exporting to the NIR (Neuromorphic Intermediate Representation) format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first load our configuration for the N-MNIST dataset. This dataset is commonly used as an easily trainable benchmark dataset, which is why we use it in this time-constrained tutorial setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"config/nmnist_feed_forward.yaml\"\n",
    "config = load_yaml(config_path)\n",
    "accelerator_config = from_metadata(\n",
    "    config.hardware_cfg,\n",
    "    config.model_cfg.network_cfg[\"quant_cfg\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the configuration, we initialize the experiment. Since we want to train a network from scratch, we don't load any checkpoint. To save time, we will be using only 5% of the dataset batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.trainer_cfg.checkpoint_path = None   # Don't load checkpoint\n",
    "config.trainer_cfg.num_epochs = 5           # Limit number of epochs\n",
    "\n",
    "trainer, model, data_module = initialize_experiment(config, limit_batches=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the network for 5 epochs (defined in the configuration) and visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=data_module, ckpt_path=None)\n",
    "\n",
    "network_logger.save_perf_log()\n",
    "draw_perf_log(network_logger.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training finishes, let's test the accuracy of our trained model (again only using 5% of the testing data). Note that we first quantize the model according to the fixed point scheme of our accelerator hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = quantize_weights(model, accelerator_config.weight_quant_scheme.wl, accelerator_config.weight_quant_scheme.fl)\n",
    "_ = trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware constraints\n",
    "\n",
    "Hardware in general poses certain requirements and constraints on what trained networks may look like. Our neuromorphic accelerator is no exception to this rule. While we have flexible fan-in and fan-out of our neurons, the total number of neurons placed on a single core is limited. Also, the total amount of synapses (i.e., network weights) is limited by the memory available in a core.\n",
    "\n",
    "These constraints need to be checked and respected by the training. Optionally, unstructured pruning can be used to tailor networks to match them. However, for the sake of time, this tutorial will not demonstrate this mechanism, but rather show how constraints might be checked and enforced in a future version of the toolchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial configuration meets all constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    fan_in_out_pruning_neccessary,\n",
    "    global_pruning_necessary,\n",
    "    num_weights_hidden,\n",
    "    input_dim,\n",
    "    hidden_features,\n",
    "    output_dim\n",
    ") = check_constraints(config, accelerator_config)\n",
    "\n",
    "print(f\"Global pruning neccessary: {global_pruning_necessary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we restrict the number of possible neurons per core, our input layer with shape of `CHW = [2, 17, 17]` and a resulting 578 input neurons is too large to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many input channels\n",
    "config.hardware_cfg[\"num_neurons_core\"] = 512\n",
    "\n",
    "accelerator_config = from_metadata(\n",
    "    config.hardware_cfg,\n",
    "    config.model_cfg.network_cfg[\"quant_cfg\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    (\n",
    "        fan_in_out_pruning_neccessary,\n",
    "        global_pruning_necessary,\n",
    "        num_weights_hidden,\n",
    "        input_dim,\n",
    "        hidden_features,\n",
    "        output_dim\n",
    "    ) = check_constraints(config, accelerator_config)\n",
    "except Exception as e:\n",
    "    print(f\"Exception occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When limiting the number of synapses per neuron core, global pruning becomes neccessary. By reducing the overall number of weights in an unstructured way, constraints could be met without loosing much accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many synapses\n",
    "config.hardware_cfg[\"num_neurons_core\"] = 1024\n",
    "config.hardware_cfg[\"num_synapses_core\"] = 16384\n",
    "config.hardware_cfg[\"num_routes\"] = 16384\n",
    "\n",
    "accelerator_config = from_metadata(\n",
    "    config.hardware_cfg,\n",
    "    config.model_cfg.network_cfg[\"quant_cfg\"]\n",
    ")\n",
    "\n",
    "(\n",
    "    fan_in_out_pruning_neccessary,\n",
    "    global_pruning_necessary,\n",
    "    num_weights_hidden,\n",
    "    input_dim,\n",
    "    hidden_features,\n",
    "    output_dim\n",
    ") = check_constraints(config, accelerator_config)\n",
    "\n",
    "print(f\"Global pruning neccessary: {global_pruning_necessary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruned networks\n",
    "\n",
    "Our hardware architecture is fully event-based, which enables it to leverage both temporal and spacial (or structural) sparsity in network architectures to reduce computation time. To show that property, we brought two pre-trained networks for the N-MNIST and the Spiking Heidelberg Digits (SHD) dataset each. The first respective network is unpruned, while the second one has been pruned by 30% (without fine-tuning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-MNIST:** for the N-MNIST dataset, the accuracy barely diminishes after pruning. This is due to the simplicity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import SuppressOutput\n",
    "\n",
    "config_path = \"config/nmnist_feed_forward.yaml\"\n",
    "config = load_yaml(config_path)\n",
    "accelerator_config = from_metadata(\n",
    "    config.hardware_cfg,\n",
    "    config.model_cfg.network_cfg[\"quant_cfg\"]\n",
    ")\n",
    "\n",
    "checkpoints = [\n",
    "    \"checkpoints/nmnist.ckpt\",\n",
    "    \"checkpoints/nmnist_30_pruning.ckpt\",\n",
    "]\n",
    "\n",
    "pruning_levels = [0.0, 0.3]\n",
    "accuracies = []\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    with SuppressOutput() as s:\n",
    "        s.print(f\"Testing checkpoint {checkpoint}\")\n",
    "        config.trainer_cfg.checkpoint_path = checkpoint\n",
    "        trainer, model, data_module = initialize_experiment(config)\n",
    "        model = quantize_weights(model, accelerator_config.weight_quant_scheme.wl, accelerator_config.weight_quant_scheme.fl)\n",
    "    \n",
    "        [test_output] = trainer.test(model, data_module)\n",
    "        accuracies.append(test_output[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No pruning:             {100 * accuracies[0]:.2f}% accuracy\")\n",
    "print(f\"Pruned (30% sparsity):  {100 * accuracies[1]:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spiking Heidelberg Digits (SHD):** when using the SHD dataset, the accuracy diminishes a lot after pruning. Since we did not fine-tune the networks, the pruned networks are not able to solve the task well, loosing ~12% of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import SuppressOutput\n",
    "\n",
    "config_path = \"config/shd_feed_forward.yaml\"\n",
    "config = load_yaml(config_path)\n",
    "accelerator_config = from_metadata(\n",
    "    config.hardware_cfg,\n",
    "    config.model_cfg.network_cfg[\"quant_cfg\"]\n",
    ")\n",
    "\n",
    "checkpoints = [\n",
    "    \"checkpoints/shd.ckpt\",\n",
    "    \"checkpoints/shd_30_pruning.ckpt\",\n",
    "]\n",
    "\n",
    "pruning_levels = [0.0, 0.3]\n",
    "accuracies = []\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    with SuppressOutput() as s:\n",
    "        s.print(f\"Testing checkpoint {checkpoint}\")\n",
    "        config.trainer_cfg.checkpoint_path = checkpoint\n",
    "        trainer, model, data_module = initialize_experiment(config)\n",
    "        model = quantize_weights(model, accelerator_config.weight_quant_scheme.wl, accelerator_config.weight_quant_scheme.fl)\n",
    "    \n",
    "        [test_output] = trainer.test(model, data_module)\n",
    "        accuracies.append(test_output[\"test_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No pruning:             {100 * accuracies[0]:.2f}% accuracy\")\n",
    "print(f\"Pruned (30% sparsity):  {100 * accuracies[1]:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to NIR\n",
    "\n",
    "After training and evaluating our SNN in software using Norse, we want to convert it into a portable, standardized format. The Neuromorphic Intermediate Representation (NIR) format is already supported by Norse (and other frameworks), which is why we also opt into using it as the intermediate format for our toolchain.\n",
    "\n",
    "The NIR export uses one sample from our data to trace the computational graph and then maps the modules in our model to standardized NIR nodes. We modified the export functions of Norse and NIRTorch (the torch plugin of NIR) in custom forks to support our custom quantized neuron implementation, but the generated NIR file still adheres to the standard defined by NIR.\n",
    "\n",
    "To capture the hardware specific configuration like memory layouts and quantization schemes, we embed that information into the metadata of the NIR file. The next step in the toolchain (the deployment phase) can read that metadata and format the memory files for the accelerator accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.trainer_cfg.checkpoint_path = \"checkpoints/shd.ckpt\"\n",
    "trainer, model, data_module = initialize_experiment(config)\n",
    "\n",
    "network_output_dir = os.path.join(config.trainer_cfg.output_path, \"networks\")\n",
    "network_output_file = os.path.join(network_output_dir, \"network.nir\")\n",
    "os.makedirs(network_output_dir, exist_ok=True)\n",
    "\n",
    "sample_data = next(iter(data_module.train_dataloader()))[0][0, 0:1, :]\n",
    "\n",
    "hardware_cfg = config.hardware_cfg\n",
    "quant_cfg = config.model_cfg.network_cfg[\"quant_cfg\"]\n",
    "metadata = {\"hardware_cfg\": hardware_cfg, \"quant_cfg\": quant_cfg}\n",
    "\n",
    "export_nir(model.network, metadata, network_output_file, sample_data, dt=config.model_cfg.network_cfg[\"dt\"], broadcast_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "As illuded to previously, this stage operates only on the generated NIR file and its embedded metadata. We parse the NIR graph and the hardware-specific information, simulate the network inference in a bit-accurate fixed-point simulation, and generate the memory files that define the topology, connectivity and weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nir\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from ecs_deploy.network import Network, generate_input_events\n",
    "from ecs_common.hardware_config import from_metadata\n",
    "from ecs_common.quant_options import options_from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the NIR graph and parse the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_output_dir = os.path.join(config.trainer_cfg.output_path, \"deploy\")\n",
    "os.makedirs(deploy_output_dir, exist_ok=True)\n",
    "\n",
    "# Load NIR graph and metadata\n",
    "nir_graph = nir.read(network_output_file)\n",
    "metadata = nir_graph.metadata\n",
    "quant_options = options_from_config(metadata[\"quant_cfg\"])\n",
    "accelerator_config = from_metadata(metadata[\"hardware_cfg\"], quant_options.weight_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate source encoded input events from one sample of the dataset. This can also be done using previously exported samples in the `.npy` format. The input events are fed into our simulator. The simulator matches our hardware implementation of a LIF neuron on the bit-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input sample and network\n",
    "test_set = data_module.test_dataloader().dataset\n",
    "total_samples = len(test_set)\n",
    "sample, _ = random_split(test_set, [1, total_samples - 1])\n",
    "sample_loader = torch.utils.data.DataLoader(sample, batch_size=1)\n",
    "sample_data, target = next(iter(sample_loader))\n",
    "sample_data = sample_data.permute([1, 0, 2, 3, 4])\n",
    "\n",
    "input_events = generate_input_events(sample_data.numpy())\n",
    "network = Network(nir_graph, accelerator_config, quant_options)\n",
    "\n",
    "# Simulate execution\n",
    "\n",
    "# The +3 is due to differing latencies between the Norse and our simulation.\n",
    "# Norse does not implement a pipeline, but rather propagates the spikes from\n",
    "# input to output within 1 timestep, while we need x timesteps (for x layers).\n",
    "# Also, one extra timestep gets added because of the core internal pipeline.\n",
    "# Hence: 1 (hidden layer) + 1 (output layer) + 1 = 3 timesteps delay.\n",
    "output_neuron_states = network.simulate(input_events, len(sample_data) + 3)\n",
    "output_neuron_states_np = np.array([[[state.to_float() for state in states_ts]] for states_ts in output_neuron_states])\n",
    "\n",
    "predicted_class = np.argmax(output_neuron_states_np[-1])\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Actual class:    {target.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate the the memory file artifacts. We also report the utilization of the accelerator memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate memory files\n",
    "network.generate_mem_files(deploy_output_dir, print_util=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate files for FPGA experiments\n",
    "\n",
    "The previous steps were quite verbose, which is why we provide an automated export function for this tutorial. It exports both the unpruned and pruned checkpoints for the N-MNIST and SHD datasets and 10 test samples (including the expected neuron state outputs) for evaluating on the FPGA later.\n",
    "\n",
    "The outputs of our bit-accurate simulation do not exactly match the Norse implementation (yet). A fully aligned Norse neuron implementation is however planned for the very near future. We report the maximum error and MSE, which both are very low. The relations between the output neurons are preserved though, and as the tasks at hand are classification tasks, this does not alter the final network output.\n",
    "\n",
    "Another interesting point to note is that the weight utilization does not decrease by 30% for the pruned network (as one might expect). This is due to the fact that we employ weight sharing in our deployment toolchain. While the total number of weight is cut by 30%, the number of unique weights is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import training_export, deployment_export\n",
    "\n",
    "print(\"Exporting N-MNIST\")\n",
    "training_export(\"nmnist\", num_samples=10)\n",
    "deployment_export(\"nmnist\")\n",
    "\n",
    "print(\"\\n\\nExporting SHD\")\n",
    "training_export(\"shd\", num_samples=10)\n",
    "deployment_export(\"shd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy to FPGA\n",
    "\n",
    "Finally, we transfer the files generated in the previous step onto the FPGA. (This will fail if you are not connected to an FPGA with the setup used in the tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import transfer_files\n",
    "transfer_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
